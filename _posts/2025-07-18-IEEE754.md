---
title: "IEEE 754, Errors, and Algorithms"
date: 2025-07-18 12:00:00 +0900

categories: [Numerical Analysis]
tags: [floating-point, stability, ieee754, kahan, double]

math: true
permalink: /posts/IEEE754/

sitemap:
  changefreq: weekly
  priority: 0.5

description: "An overview of IEEE 754 representation (single and double), common error types, and stable numerical algorithms such as Kahan summation etc."
author: "JIN HO JEON"
---
 


## 1. Intro

Before the IEEE 754 standard, floating-point number representations varied across manufacturers.  
This caused inconsistencies in numerical computations across systems.   
To address this issue, a standard was proposed in collaboration with hardware companies like Intel and Motorola and numerical analysts such as **William Kahan** who is now a prominent figure in this field. Before diving into precision algorithms, we should first cover the IEEE 754 floating‑point representation--including both normal and subnormal numbers--
and various error modes inherent in IEEE 754 floating-point systems, such as rounding error and overflow.

### 1.1 What is IEEE 754?

The IEEE 754 standard defines:

- Binary formats ( e.g., float = 32 bits, double = 64 bits, etc.)
- Layout of sign / exponent / mantissa (fraction)
- Rules for rounding, overflow, underflow, NaNs, and infinities

It became the **default representation** for floating-point arithmetic in most modern computing systems.


IEEE 754 represents numbers in the form:

$$
x = (-1)^s \times (1.f) \times 2^{e - \text{bias}} 
$$

the value of bias depends on whether it is single or double precision. 
if it is double precision, then the bias is 1023; otherwise, it's 127.



| Component  | Description                        |
|------------|------------------------------------|
| Sign bit   | 0 for positive, 1 for negative     |
| Exponent   | Encodes magnitude (biased format)  |
| Mantissa   | Fractional bits after binary point |



### 1.2. Float vs Double: Bit-level Differences

IEEE 754 defines the formats as follows:

| Type   | Total Bits | Sign | Exponent | Mantissa |
|--------|------------|------|----------|----------|
| float  | 32         | 1    | 8        | 23       |
| double | 64         | 1    | 11       | 52       |

Mathematically:

- Float:
  $$
  x = \pm (1.a _ 1 a_  2 \dots a_{23}) \times 2^{E-bias}, bias = 127
  $$

- Double:
  $$
  x = \pm (1.a _ 1 a _ 2 \dots a_{52}) \times 2^{E-bias}, bias = 1023
  $$
 
---

## 2. Rounding Errors and Two Type of Errors 

### 2.1 Machine epsilon and relative, absolute error 

Suppose $fl(x)$ represents the value of x in computer system.

- relative error 

$$
	\epsilon_{relative} = \frac{|x - fl(x)|}{|x|}
$$

- absolute error 

$$
	\epsilon_{absolute} = |x - fl(x)|
$$ 

### 2.2 Error Types and Exceptions  

- truncation error : this error arises from truncating an infinite decimal 

In short, it's originated from 

- rounding-off error : 

- overflow/underflow : To put it directly, it happens when your result is beyond limitation of  
- negligible error
 when you add two numbers, called x, y with  $|x-y|>2^{52}$, then, the smaller one is omiited. wlog, y is smaller than x. Then, fl(x+y) = fl(x). 

- loss of significance : Loss of significance occurs when subtracting two nearly equal numbers. 

Consider $f_{1}(x) = \sqrt{x} * (\sqrt{x+1} - \sqrt{x})$,  $f_{2}(x) = \frac{\sqrt{x}}{\sqrt{x+1}-\sqrt{x}}$.

They are mathematically equal. However, when it comes to calculation, the result could be different. 

It's because 

```python

import decimal
from decimal import Decimal
import math 

decimal.getcontext().prec = 50


f1 = lambda x : Decimal(x).sqrt()*((Decimal(x+1)).sqrt()-(Decimal(x)).sqrt())
f2 = lambda x : Decimal(x).sqrt()/((Decimal(x+1)).sqrt()+(Decimal(x)).sqrt())


for i in range(0,54):
    print(f"at {pow(10,i):d}, \t f1(x) = {f1(pow(10,i))} \t f2(x) = {f2(pow(10,i))}")


```
``` python
# output at 
at 100000000000000000000000000000000000000000000000000000,      f1(x) = 0E-46   f2(x) = 0.5
```

- error magnification : 

- computation error : this error is derived from computational algorithm or iterations. 



### 2.3 Examples 
- 
Floating-point operations are not associative due to rounding:


```python

# Example in Python
a = 1e20
b = -1e20
c = 3.14

print((a + b) + c)  # prints 3.14
print(a + (b + c))  # prints 0.0
```

## 3. Precision Algorithms 

- Kahan Two Sum
 This algorithm can guarantee $O(n\epsilon)$ about n summation terms.
 Let's think of this why.

```python

def KahanTwoSum(x:[list:float]):
  c = 0.0 # current error 
  t = 0.0 # global sum 
  for x_ele in x:
    y = x_ele - c
    total = t + y # an error might exists + e1 
    c = (total - t) - y
    t = total 
```

- Neumaier (1974)
  This is invented for addressing the error gap when using Kahan Two Sum.

```python 

def Neumaier(x:[list:float]):
  c = 0.0
  t = 0.0
  for x_ele in x:
    total = t + x_ele # f(t + x_ele) = t + x_ele + $\epsilon$
    if abs(total) >= abs(x_ele):
      c += (t - total) - x_ele  
    else:
      c += (x_ele - total) + t
    t = total
  return t + c
```
  we can expect $O(n\epsilon)$
  
  Proof.
		
- Superaccumulator

    
```python



import struct

def float_to_bits(x:float):
    
    bits = struct.unpack('>Q', struct.pack('>d',x))[0]
    signed_bit = ((bits >> 63) & 1)
    exponent = ((bits >>52) & (0x7FF) )
    mantissa = (bits & ( (1<<52) -1))
    return signed_bit, exponent, mantissa

def superacc(x_list:list[float], bins = 2048):

    acc_neg = [0.0] * bins
    acc_pos = [0.0] * bins
    offset = bins//2
    bias = 1023

    for x in x_list:
        sign_bit, expo, mantissa = float_to_bits(x)

        if expo  == 2047:
            if mantissa == 0:
                return float('inf')
            else:
                return float('nan')


        if expo == 0: #subnormal
            precision = 52
            man = mantissa
            e = 1 - bias
        # this place for NaN and Infty
        else: # normal
            precision = 53
            e = expo - bias
            man = (1<< 52) | mantissa

        for i in range(precision):
            idx = e + offset - i
            if (( man >> (precision-i-1)) & 1) == 1:
                if 0 <= idx < bins:
                    if sign_bit == 0: # positive
                        acc_pos[idx] +=1
                    else:
                        acc_neg[idx] +=1

    # in this case, the exponents are already aligned,
    # so normalization is not required.

    result = 0.0
    for i in range(bins):
        num = acc_pos[i] - acc_neg[i]
        e = i - offset
        result += (num)*2**(e)
        
    print(result)

    
```

and there are much more sophisticated algorithms divided into two ways; by software or hardware. 


